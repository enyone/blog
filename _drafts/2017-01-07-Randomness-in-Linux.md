Majority of web pages and blog posts I've read suggests to use `/dev/urandom`.
I would say this is stuppid advice.

Doing random things on purpose is very difficult. Especially with electronics. Things
may appear fully random but often are not in fact. Still randomness is key part in
important practices like cryptography. Thus it is important to understand terms
**random** and **randomness**.

end of TL;DR so now go and read...

Sources of randomness
---

> *Randomness is the lack of pattern or predictability in events.*

> *Wikipedia*

There are three generally accepted sources of randomness:
* environment the system is working in
* initial conditions of the system
* things generated by the system itself

Randomness generated by the system itself is called **pseudo**randomness. There is no
mathematical source of true randomness.

The power of random things is that those things are really unpredictable. In truly
random events no one is able to predict the nature of next event. These events can be
random in time, intensity, but often called as being stochastic.

> *https://en.wikipedia.org/wiki/Stochastic*

Randomness in computer systems
---

Everyone should know this meme from xkcd:

```java
int getRandomNumber()
{
  return 4 // chosen by fair dice roll
           // guaranteed to be random
}
```
> *http://xkcd.com/221/*

While its output being truly random above function is still quite unusable when
unpredictability is needed and unpredictability is always needed.

In computer system high enough unpredictability is often as useful as full
unpredictability. It is only a matter of energy (or value $$$) needed to predict
next event. As long as energy needed for predicting things is higher than the
value of the achievement of successful prediction we can say system is
unpredictable enough.

Also time should be taken into account because calculating power comes cheaper day by
day. Things that are too expensive to predict today can be cheap enough tomorrow.
Thus unpredictable system must stay unpredictable enough also in the future, not
forever but long enough in relation to value.

*This is pure analogy to information security as data has always a value and if
it is encrypted using algorithms that are relaying randomness (and most often are)
it is only a matter of time and energy when the random event behind this encryption
will be predicted and data to become public.*

Randomness in operating systems
---

Instead of starting to create your own pseudorandom generator nearly all operating
systems provide the source of pseudorandomness.

**GNU/Linux**

Linux kernel maintains an **entropy pool**. From where data to this pool is obtained, you can start reading:

[/drivers/char/random.c](https://github.com/torvalds/linux/blob/master/drivers/char/random.c)

*There is that `push_to_pool()` function which is called from various locations in the kernel.
I'm not going into details on this but will say that at least mouse movements and keyboard presses
are used when pushing environment related random bits to pool.*

To check how much data is available in this pool:
```sh
# This read-only file gives the available entropy.
# Normally, this will be 4096 (bits), a full entropy pool.
cat /proc/sys/kernel/random/entropy_avail
859
```

Every time when the content of this entropy pool is used to derive pseudorandom data that content is
removed from the pool and more content needs to be pushed back with `push_to_pool()` to maintain decent
pool size.

**/dev/random**

Linux kernel provides you pseudorandom data derived from the content of this entropy pool. There is
file called `/dev/random` which is not a static file but a file provided by the kernel itself. Every
time you read from this file kernel will give you pseudorandom data derived from the content of its
entropy pool.

```sh
# Read first 10 bytes and present as base64 encoded
head -c 10 /dev/random | base64 
xlQ+BV9CMUJbnw==
```

There is one nasty behaviour of `/dev/random`; every time entropy pool drains itself empty all read
operations to `/dev/random` will block (io-wait) until enough data is pushed and becoming available
from the entropy pool to derive more random data.

The quality (unpredictability) of pseudorandomness is better when blocking until enough data is pushed
and becoming available from the entropy pool.

**/dev/urandom**

When **lower quality** (more predictable) pseudorandom data can be used there is `/dev/urandom` available
for that purpose. Read operations to this file will not block (io-wait) when entropy pool drains
itself empty. Instead kernel will create **lower quality** pseudorandom data to substitute better quality
data derived from the content of its entropy pool.

**Which one to use?**

Majority of web pages and blog posts I've read suggests to use `/dev/urandom`. I would say this is stuppid advice.

When the best available quality is needed `/dev/random` should be used always even with the cost of blocking.
