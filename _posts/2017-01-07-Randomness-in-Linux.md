# Randomness in Linux

Majority of web pages and blog posts I've read suggests to use `/dev/urandom`.

I would say this is stupid advice.

Doing random things on purpose is very difficult. Especially with electronics. Things
may appear fully random but often are not in fact. Still randomness is key part in
important practices like cryptography. Thus it is important to understand terms
**random** and **randomness**.

end of TL;DR so now go and read...

Sources of randomness
---

> *Randomness is the lack of pattern or predictability in events. -Wikipedia*

There are three generally accepted sources of randomness:
* environment the system is working in
* initial conditions of the system
* things generated by the system itself

Randomness generated by the system itself is called **pseudo**-randomness. There is no
mathematical source of true randomness.

The power of random things is that those things are really unpredictable. In truly
random events no one is able to predict the nature of next event. These events can be
random in time, intensity, but often called as being [stochastic](https://en.wikipedia.org/wiki/Stochastic).

Randomness in computer systems
---

Everyone should know this meme from xkcd:

```java
# from http://xkcd.com/221/
int getRandomNumber()
{
  return 4 // chosen by fair dice roll
           // guaranteed to be random
}
```

While its output being truly random above function is still quite unusable when
unpredictability is needed and unpredictability is always needed.

In computer system high enough unpredictability is often as useful as full
unpredictability. It is only a matter of energy (or value $$$) needed to predict
next event. As long as energy needed for predicting things is higher than the
value of the achievement of successful prediction we can say system is
unpredictable enough.

Also time should be taken into account because calculating power comes cheaper day by
day. Things that are too expensive to predict today can be cheap enough tomorrow.
Thus unpredictable system must stay unpredictable enough also in the future, not
forever but long enough in relation to value.

*This is pure analogy to information security as data has always a value and if
it is encrypted using algorithms that are relaying randomness (and most often are)
it is only a matter of time and energy when the random event behind this encryption
will be predicted and data to become public.*

Randomness in operating systems
---

Instead of starting to create your own pseudo-random generator nearly all operating
systems provide the source of pseudo-randomness.

Randomness in GNU/Linux
---

Linux kernel maintains an **[entropy](https://en.wikipedia.org/wiki/Entropy) pool**. From where data to this pool is obtained, you can start reading:

[/drivers/char/random.c](https://github.com/torvalds/linux/blob/master/drivers/char/random.c)

*There is that `push_to_pool()` function which is called from various locations in the kernel.
I'm not going into details on this but will say that at least mouse movements and keyboard presses
are used when pushing environment related true random bits into pool.*

To check how much bits are available in this pool:

```sh
# This read-only file gives the available entropy.
# Normally, this will be 4096 (bits), a full entropy pool.
cat /proc/sys/kernel/random/entropy_avail
859
```

Every time when the content of this entropy pool is used to derive pseudo-random data that content is
removed from the pool and more content needs to be pushed back with `push_to_pool()` to maintain decent
pool size.

External random sources
---

Using [different software utilities](https://wiki.archlinux.org/index.php/Rng-tools) Linux kernel can become
aware of [external hardware](https://en.wikipedia.org/wiki/Hardware_random_number_generator) random bit sources and use these as a source of the entropy pool itself. This
provides even better entropy in the pool and thus to create better (more unpredictable) pseudo-random data.

The case of /dev/random
---

Linux kernel provides you pseudo-random data derived from the content of this entropy pool. There is
file called `/dev/random` which is not a static file but a file provided by the kernel itself. Every
time you read from this file kernel will give you pseudo-random data derived from the content of its
entropy pool.

```sh
# Read first 10 bytes and present as base64 encoded
head -c 10 /dev/random | base64 
xlQ+BV9CMUJbnw==
```

There is one nasty behavior of `/dev/random`; every time entropy pool drains itself empty all read
operations to `/dev/random` will **block** (io-wait) until enough data is pushed and becoming available
from the entropy pool to derive more random data.

The quality (unpredictability) of pseudo-randomness is better when blocking until enough data is pushed
and becoming available from the entropy pool.

The case of /dev/urandom
---

When **lower quality** (more predictable) pseudo-random data can be used there is `/dev/urandom` available
for that purpose. Data is again derived from the content of this entropy pool but read operations to this
file will not block (io-wait) when entropy pool drains itself empty. Instead kernel will create
**lower quality** pseudo-random data to substitute better quality data derived from the content of its
entropy pool. This is achieved by re-using already used bits obtained from entropy pool.

Which one to use?
---

Majority of web pages and blog posts I've read suggests to use `/dev/urandom`. I would say this is stupid advice.

When the best available quality is needed `/dev/random` should be used always even with the cost of blocking.
One example is when generating keys **to be used in cryptographic algorithms**.

When there is the absolute need to be non-blocking even with the cost of lower quality randomness `/dev/urandom` can be used. But still be very careful when making the decision.
